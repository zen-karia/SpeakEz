{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3be5b6f-ad29-43e3-8ebe-e9584e6ea340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loaded 316 samples from 26 classes\n",
      "Epoch 1/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.0626 - loss: 3.3279 - val_accuracy: 0.1562 - val_loss: 3.0278\n",
      "Epoch 2/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1643 - loss: 2.9927 - val_accuracy: 0.1875 - val_loss: 2.7982\n",
      "Epoch 3/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2270 - loss: 2.8224 - val_accuracy: 0.3281 - val_loss: 2.5975\n",
      "Epoch 4/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3139 - loss: 2.5577 - val_accuracy: 0.3438 - val_loss: 2.4001\n",
      "Epoch 5/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3294 - loss: 2.4777 - val_accuracy: 0.4062 - val_loss: 2.1889\n",
      "Epoch 6/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4595 - loss: 2.1571 - val_accuracy: 0.5000 - val_loss: 1.9844\n",
      "Epoch 7/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4995 - loss: 2.0691 - val_accuracy: 0.5938 - val_loss: 1.7870\n",
      "Epoch 8/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5634 - loss: 1.8543 - val_accuracy: 0.6562 - val_loss: 1.5846\n",
      "Epoch 9/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6761 - loss: 1.6014 - val_accuracy: 0.6875 - val_loss: 1.4060\n",
      "Epoch 10/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7349 - loss: 1.3385 - val_accuracy: 0.8438 - val_loss: 1.2291\n",
      "Epoch 11/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7341 - loss: 1.2487 - val_accuracy: 0.8750 - val_loss: 1.0570\n",
      "Epoch 12/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7657 - loss: 1.1253 - val_accuracy: 0.8750 - val_loss: 0.9185\n",
      "Epoch 13/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8359 - loss: 0.9199 - val_accuracy: 0.8906 - val_loss: 0.7998\n",
      "Epoch 14/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8118 - loss: 0.8355 - val_accuracy: 0.8750 - val_loss: 0.7172\n",
      "Epoch 15/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8501 - loss: 0.7818 - val_accuracy: 0.8906 - val_loss: 0.6244\n",
      "Epoch 16/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8692 - loss: 0.7033 - val_accuracy: 0.9062 - val_loss: 0.5688\n",
      "Epoch 17/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8817 - loss: 0.6087 - val_accuracy: 0.9375 - val_loss: 0.5210\n",
      "Epoch 18/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9374 - loss: 0.4809 - val_accuracy: 0.9219 - val_loss: 0.4723\n",
      "Epoch 19/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9473 - loss: 0.4895 - val_accuracy: 0.9219 - val_loss: 0.4416\n",
      "Epoch 20/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9095 - loss: 0.4639 - val_accuracy: 0.9219 - val_loss: 0.4096\n",
      "Epoch 21/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9261 - loss: 0.4713 - val_accuracy: 0.9219 - val_loss: 0.3905\n",
      "Epoch 22/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9417 - loss: 0.3588 - val_accuracy: 0.9375 - val_loss: 0.3668\n",
      "Epoch 23/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9297 - loss: 0.3989 - val_accuracy: 0.9219 - val_loss: 0.3615\n",
      "Epoch 24/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9591 - loss: 0.3314 - val_accuracy: 0.9375 - val_loss: 0.3466\n",
      "Epoch 25/25\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9674 - loss: 0.2888 - val_accuracy: 0.9219 - val_loss: 0.3194\n",
      "✅ Training complete. Model + scaler + label encoder saved.\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1) CONFIG\n",
    "# DATASET_DIR = \"File-Path\"\n",
    "IMG_EXTS   = (\"*.jpg\", \"*.png\")\n",
    "MODEL_PATH = \"asl_letter_model.keras\"\n",
    "SCALER_PATH = \"scaler.pkl\"\n",
    "LE_PATH    = \"label_encoder.pkl\"\n",
    "\n",
    "# 2) EXTRACT FEATURES & LABELS\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5)\n",
    "data, labels = [], []\n",
    "\n",
    "for class_name in sorted(os.listdir(DATASET_DIR)):\n",
    "    class_dir = os.path.join(DATASET_DIR, class_name)\n",
    "    if not os.path.isdir(class_dir): continue\n",
    "    for ext in IMG_EXTS:\n",
    "        for img_path in glob.glob(os.path.join(class_dir, ext)):\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None: continue\n",
    "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            res = mp_hands.process(rgb)\n",
    "            if not res.multi_hand_landmarks: continue\n",
    "            lm = res.multi_hand_landmarks[0].landmark\n",
    "            feat = []\n",
    "            for p in lm:\n",
    "                feat += [p.x, p.y, p.z]\n",
    "            data.append(feat)\n",
    "            labels.append(class_name.upper())  # ensure uppercase A–Z\n",
    "\n",
    "mp_hands.close()\n",
    "X = np.array(data, dtype=np.float32)          # shape (N,63)\n",
    "y = np.array(labels)                          # shape (N,)\n",
    "\n",
    "print(f\">> Loaded {len(X)} samples from {len(set(y))} classes\")\n",
    "\n",
    "# 3) ENCODE LABELS & SCALE FEATURES\n",
    "le = LabelEncoder().fit(y)\n",
    "y_enc = le.transform(y)                       # 0–26 labels\n",
    "y_ohe = tf.keras.utils.to_categorical(y_enc)  # one-hot to (N,27)\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# 4) SPLIT\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y_ohe, test_size=0.2, random_state=42, stratify=y_ohe)\n",
    "\n",
    "# 5) BUILD & TRAIN MODEL\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(63,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=25,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# 6) SAVE EVERYTHING\n",
    "model.save(MODEL_PATH)\n",
    "with open(SCALER_PATH, \"wb\") as f: pickle.dump(scaler, f)\n",
    "with open(LE_PATH,    \"wb\") as f: pickle.dump(le,    f)\n",
    "\n",
    "print(\"✅ Training complete. Model + scaler + label encoder saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6b1c5-52dd-4e96-8fd0-3c36df485dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
